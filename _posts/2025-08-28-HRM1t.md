---
layout: post
title: "Implementing a Hierarchical Reasoning Model for Regression"
date: 2025-08-28
---

📄 *English*



Recently I started experimenting with the **𝗛𝗶𝗲𝗿𝗮𝗿𝗰𝗵𝗶𝗰𝗮𝗹 𝗥𝗲𝗮𝘀𝗼𝗻𝗶𝗻𝗴 𝗠𝗼𝗱𝗲𝗹 (𝗛𝗥𝗠)**, a new architecture introduced by Sapient Intelligence on arXiv:
https://lnkd.in/dM3ZsH7y



→ What is **𝗛𝗥𝗠**?



It’s inspired by how the brain reasons: a high-level module (planner, slower and abstract) works together with a low-level module (executor, faster and detailed). The hierarchy is learned from data, not handcrafted. In the original paper, HRM also integrates a stop signal (Adaptive Computation Time) so the model learns when to stop reasoning dynamically.



→ Our adaptation to tabular regression



We built a simplified PyTorch version with:
Inputs: geospatial features + embeddings from AlphaEarth (a Google DeepMind model for global environmental monitoring).
Architecture: high-level GRU → plan module → low-level GRU → output head.
Training: AdamW (lr=3e-4), regularization, gradient clipping, 40 epochs, batch size 512.



📊 **Initial results**
Test: R²=0.6700



**Compared with classical models:**
GBM: R²=0.8587
Random Forest: R²=0.9621



➡ Despite HRM’s potential, tree-based models still outperform it on tabular data.
What I value most is the process: adapting a frontier model to a new domain, realizing what’s missing (like the stop signal), and seeing how “old” methods remain state-of-the-art in the right context.




📄 *Español*



Hace poco me lancé a experimentar con el **𝗛𝗶𝗲𝗿𝗮𝗿𝗰𝗵𝗶𝗰𝗮𝗹 𝗥𝗲𝗮𝘀𝗼𝗻𝗶𝗻𝗴 𝗠𝗼𝗱𝗲𝗹 (𝗛𝗥𝗠)**, publicado recientemente por Sapient Intelligence en arXiv.



→ ¿Qué es el **𝗛𝗥𝗠**?



 Es un modelo inspirado en cómo razona el cerebro: combina un módulo de alto nivel (planner, más abstracto y lento) con un módulo de bajo nivel (executor, más detallado y rápido). Lo interesante es que la jerarquía no se programa a mano, sino que se aprende de los datos. En el paper original, además, incorporan un stop signal (Adaptive Computation Time) que permite al modelo decidir cuántos pasos de razonamiento necesita antes de detenerse.



→ Nuestra adaptación a regresión tabular
 Implementamos una versión simplificada en PyTorch, con:
Inputs: variables geoespaciales + embeddings de AlphaEarth (un modelo de Google DeepMind para monitoreo medioambiental a escala global).
Arquitectura: GRU de alto nivel → planificador → GRU de bajo nivel → capa de salida.
Entrenamiento: AdamW (lr=3e-4), regularización, clipping de gradiente, 40 épocas, batch 512.



📊 **Resultados iniciales**
Test: R²=0.6700



**Comparado con modelos clásicos:**
GBM: R²=0.8587
Random Forest: R²=0.9621



➡ A pesar del potencial del HRM, los modelos de árboles siguen dominando en datos tabulares.
Lo valioso fue el proceso: adaptar un modelo de frontera a un dominio distinto, entender qué piezas faltan (como el stop signal), y descubrir cómo lo clásico sigue siendo competitivo.

