---
layout: post
title: "Implementing a Hierarchical Reasoning Model for Regression"
date: 2025-08-28
---

ğŸ“„ *English*



Recently I started experimenting with the **ğ—›ğ—¶ğ—²ğ—¿ğ—®ğ—¿ğ—°ğ—µğ—¶ğ—°ğ—®ğ—¹ ğ—¥ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ ğ— ğ—¼ğ—±ğ—²ğ—¹ (ğ—›ğ—¥ğ— )**, a new architecture introduced by Sapient Intelligence on arXiv:
https://lnkd.in/dM3ZsH7y



â†’ What is **ğ—›ğ—¥ğ— **?



Itâ€™s inspired by how the brain reasons: a high-level module (planner, slower and abstract) works together with a low-level module (executor, faster and detailed). The hierarchy is learned from data, not handcrafted. In the original paper, HRM also integrates a stop signal (Adaptive Computation Time) so the model learns when to stop reasoning dynamically.



â†’ Our adaptation to tabular regression



We built a simplified PyTorch version with:
Inputs: geospatial features + embeddings from AlphaEarth (a Google DeepMind model for global environmental monitoring).
Architecture: high-level GRU â†’ plan module â†’ low-level GRU â†’ output head.
Training: AdamW (lr=3e-4), regularization, gradient clipping, 40 epochs, batch size 512.



ğŸ“Š **Initial results**
Test: RÂ²=0.6700



**Compared with classical models:**
GBM: RÂ²=0.8587
Random Forest: RÂ²=0.9621



â¡ Despite HRMâ€™s potential, tree-based models still outperform it on tabular data.
What I value most is the process: adapting a frontier model to a new domain, realizing whatâ€™s missing (like the stop signal), and seeing how â€œoldâ€ methods remain state-of-the-art in the right context.




ğŸ“„ *EspaÃ±ol*



Hace poco me lancÃ© a experimentar con el **ğ—›ğ—¶ğ—²ğ—¿ğ—®ğ—¿ğ—°ğ—µğ—¶ğ—°ğ—®ğ—¹ ğ—¥ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ ğ— ğ—¼ğ—±ğ—²ğ—¹ (ğ—›ğ—¥ğ— )**, publicado recientemente por Sapient Intelligence en arXiv.



â†’ Â¿QuÃ© es el **ğ—›ğ—¥ğ— **?



 Es un modelo inspirado en cÃ³mo razona el cerebro: combina un mÃ³dulo de alto nivel (planner, mÃ¡s abstracto y lento) con un mÃ³dulo de bajo nivel (executor, mÃ¡s detallado y rÃ¡pido). Lo interesante es que la jerarquÃ­a no se programa a mano, sino que se aprende de los datos. En el paper original, ademÃ¡s, incorporan un stop signal (Adaptive Computation Time) que permite al modelo decidir cuÃ¡ntos pasos de razonamiento necesita antes de detenerse.



â†’ Nuestra adaptaciÃ³n a regresiÃ³n tabular
 Implementamos una versiÃ³n simplificada en PyTorch, con:
Inputs: variables geoespaciales + embeddings de AlphaEarth (un modelo de Google DeepMind para monitoreo medioambiental a escala global).
Arquitectura: GRU de alto nivel â†’ planificador â†’ GRU de bajo nivel â†’ capa de salida.
Entrenamiento: AdamW (lr=3e-4), regularizaciÃ³n, clipping de gradiente, 40 Ã©pocas, batch 512.



ğŸ“Š **Resultados iniciales**
Test: RÂ²=0.6700



**Comparado con modelos clÃ¡sicos:**
GBM: RÂ²=0.8587
Random Forest: RÂ²=0.9621



â¡ A pesar del potencial del HRM, los modelos de Ã¡rboles siguen dominando en datos tabulares.
Lo valioso fue el proceso: adaptar un modelo de frontera a un dominio distinto, entender quÃ© piezas faltan (como el stop signal), y descubrir cÃ³mo lo clÃ¡sico sigue siendo competitivo.

